# QuARA LLM Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# SiliconFlow Configuration (Recommended)
# =============================================================================
SILICONFLOW_API_KEY=sk-kozafasriejhrdpdjcautgzvqdiobnhjotqmtanfnbodrzec
SILICONFLOW_API_ENDPOINT=https://api.siliconflow.cn/v1/chat/completions
SILICONFLOW_MODEL=deepseek-ai/DeepSeek-V3.2-Exp

# =============================================================================
# Default LLM Provider Configuration
# =============================================================================
# Primary provider for reasoning, tool calls, and general responses
DEFAULT_LLM_PROVIDER=siliconflow
DEFAULT_MODEL=deepseek-ai/DeepSeek-V3.2-Exp

# Code generation provider (for statistical analysis, data processing, etc.)
CODE_LLM_PROVIDER=siliconflow
CODE_LLM_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

# =============================================================================
# Provider Settings (siliconflow, openai, anthropic, custom)
# =============================================================================
QUARA_LLM_PROVIDER=siliconflow
QUARA_LLM_MODEL=deepseek-ai/DeepSeek-V3.2-Exp

# API Keys
# QUARA_API_KEY=your_api_key_here  # Generic key name
# OPENAI_API_KEY=your_openai_key_here
# ANTHROPIC_API_KEY=your_anthropic_key_here

# API Base URLs (optional, defaults provided)
QUARA_API_BASE=https://api.siliconflow.cn/v1
# SILICONFLOW_API_BASE=https://api.siliconflow.cn/v1

# =============================================================================
# Model-Specific Configuration
# =============================================================================

# Reasoning Model (for agent thinking, tool selection, decision making)
REASONING_MODEL=deepseek-ai/DeepSeek-V3.2-Exp
REASONING_TEMPERATURE=0.7
REASONING_MAX_TOKENS=4096

# Code Generation Model (for Python/R/SQL generation, data analysis)
CODE_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct
CODE_TEMPERATURE=0.3
CODE_MAX_TOKENS=8192

# Writing Model (for manuscript generation, documentation)
WRITING_MODEL=deepseek-ai/DeepSeek-V3.2-Exp
WRITING_TEMPERATURE=0.8
WRITING_MAX_TOKENS=8192

# =============================================================================
# General LLM Parameters
# =============================================================================
QUARA_TEMPERATURE=0.7
QUARA_MAX_TOKENS=4096
QUARA_TIMEOUT=60
QUARA_MAX_RETRIES=3

# =============================================================================
# System Configuration
# =============================================================================
QUARA_PROJECT_ROOT=./research_projects
QUARA_ENABLE_MEMORY=true
QUARA_LOG_LEVEL=INFO
