#!/usr/bin/env python3
"""
QuARA CLI - Interactive Command-Line Interface
================================================

A user-friendly CLI for interacting with the QuARA multi-agent research system.

Usage:
    python cli.py                    # Interactive mode
    python cli.py research "question"  # Direct research
    python cli.py chat               # Chat with LLM
    python cli.py config             # Show configuration
"""

import asyncio
import sys
import os
from pathlib import Path
from typing import Optional
import argparse

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Load environment variables
try:
    from dotenv import load_dotenv
    # Try multiple locations
    env_paths = [
        Path(__file__).parent / '.env',  # quara/.env
        Path(__file__).parent.parent / '.env',  # Scientifique/.env
    ]
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)
            break
except ImportError:
    pass

from quara import QuARASystem
from quara.utils import (
    create_llm_client,
    ModelPurpose,
    validate_research_request,
    estimate_research_time
)
from quara.core.automated_executor import AutomatedResearchExecutor


class QuARACLI:
    """Interactive CLI for QuARA system"""
    
    def __init__(self):
        self.system = None
        self.llm_client = None
        self.auto_executor = None
        self.running = True
        
    def print_banner(self):
        """Print welcome banner"""
        banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘    â•”â•â•— â•¦ â•¦ â•”â•â•— â•¦â•â•— â•”â•â•—    QuARA - Quantitative Academic Research Agent     â•‘
â•‘    â•‘â•â•¬â•—â•‘ â•‘ â• â•â•£ â• â•¦â• â• â•â•£    Multi-Agent Research Assistant                   â•‘
â•‘    â•šâ•â•â•šâ•šâ•â• â•© â•© â•©â•šâ• â•© â•©    Version 1.0                                       â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Welcome to QuARA! Type 'help' for available commands or 'exit' to quit.
"""
        print(banner)
    
    def print_help(self):
        """Print help message"""
        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Available Commands                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Research Commands: âš ï¸  Note: Full workflow is simulated (no real artifacts yet)
  research <question>     - Simulate full research workflow
  auto-research <question> - ğŸ”¥ AUTOMATED: Download data, run code, create plots!
  validate <question>     - Validate a research question
  estimate <question>     - Estimate time for research

LLM Commands: âœ… Fully Functional
  chat                    - Start chat session with LLM
  ask <question>          - Ask a single question to LLM
  code <task>             - Generate code (uses Qwen-Coder)
  write <topic>           - Generate academic content (uses DeepSeek)
  analyze <topic>         - Get detailed analysis with reasoning

System Commands:
  config                  - Show system configuration
  models                  - Show available models
  test                    - Test LLM connection
  clear                   - Clear screen

General:
  help                    - Show this help message
  exit/quit               - Exit QuARA CLI

ğŸ’¡ Debug Mode:
  Use --debug flag to enable detailed execution logging:
    python quara/cli.py --debug
  Then run auto-research to see all debug logs during execution.

Recommended Commands (Working):
  auto-research <question> ğŸ”¥ - Fully automated data + code + plots!
  ask Explain the US-China relationship's impact on financial markets
  code Write Python to analyze SPY returns correlation with geopolitical events
  write Literature review on geopolitical risk and stock market returns
  chat (for interactive discussion)

Examples:
  python quara/cli.py --debug    # Enable debug mode (interactive)
  ask What factors in US-China relations affect stock markets?
  code Create a function to download and analyze SPY historical data
  write Methods section for geopolitical risk analysis study
"""
        print(help_text)
    
    def print_config(self):
        """Print current configuration"""
        print("\n" + "="*80)
        print("  QuARA System Configuration")
        print("="*80)
        
        api_key = os.getenv("SILICONFLOW_API_KEY")
        if api_key:
            print(f"\nâœ… API Key: {api_key[:15]}...{api_key[-4:]}")
        else:
            print("\nâŒ API Key: Not configured")
        
        print(f"\nğŸŒ Provider Settings:")
        print(f"   Provider: {os.getenv('DEFAULT_LLM_PROVIDER', 'siliconflow')}")
        print(f"   API Base: {os.getenv('QUARA_API_BASE', 'https://api.siliconflow.cn/v1')}")
        
        print(f"\nğŸ¤– Model Configuration:")
        print(f"   Reasoning: {os.getenv('REASONING_MODEL', 'deepseek-ai/DeepSeek-V3.2-Exp')}")
        print(f"   Code:      {os.getenv('CODE_MODEL', 'Qwen/Qwen2.5-Coder-32B-Instruct')}")
        print(f"   Writing:   {os.getenv('WRITING_MODEL', 'deepseek-ai/DeepSeek-V3.2-Exp')}")
        
        print(f"\nâš™ï¸  Parameters:")
        print(f"   Timeout: {os.getenv('QUARA_TIMEOUT', '180')}s")
        print(f"   Max Retries: {os.getenv('QUARA_MAX_RETRIES', '3')}")
        print(f"   Project Root: {os.getenv('QUARA_PROJECT_ROOT', './research_projects')}")
        print()
    
    async def test_connection(self):
        """Test LLM connection"""
        print("\nğŸ”§ Testing LLM connection...")
        
        try:
            if not self.llm_client:
                self.llm_client = create_llm_client()
            
            print("âœ… LLM client initialized")
            
            # Test simple generation
            print("\nğŸ“¡ Sending test request...")
            response = await self.llm_client.generate(
                "Say 'Hello from QuARA!' in one sentence.",
                temperature=0.5
            )
            
            print(f"\nâœ… Connection successful!")
            print(f"ğŸ“ Response: {response}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ Connection failed: {e}")
            return False
    
    async def validate_question(self, question: str):
        """Validate research question"""
        print(f"\nğŸ” Validating: {question}")
        
        result = validate_research_request(question)
        
        print(f"\n{'âœ…' if result['valid'] else 'âŒ'} Validation Result:")
        print(f"   Valid: {result['valid']}")
        print(f"   Word Count: {result.get('word_count', 0)}")
        print(f"   Has Research Indicators: {result.get('has_research_indicators', False)}")
        
        if result['valid']:
            time_est = estimate_research_time(question)
            print(f"\nâ±ï¸  Estimated Time: {time_est}")
    
    async def conduct_research(self, question: str):
        """Conduct research (currently simulated)"""
        print(f"\nğŸ”¬ Research Request: {question}")
        print("="*80)
        print("\nâš ï¸  Note: Full research workflow is currently simulated.")
        print("    Real artifact generation is under development.")
        print("\nğŸ’¡ For working analysis, try:")
        print(f"   â€¢ ask {question[:50]}...")
        print(f"   â€¢ analyze {question[:50]}...")
        print(f"   â€¢ code <programming task related to your question>")
        print("\nWould you like to:")
        print("  1. Continue with simulation")
        print("  2. Get comprehensive analysis instead (recommended)")
        print("  3. Cancel")
        
        # For non-interactive use, proceed with simulation
        print("\nğŸ“¦ Initializing QuARA system...")
        
        try:
            # Initialize system if needed
            if not self.system:
                self.system = QuARASystem()
                await self.system.start_system()
            
            # Conduct research
            print("\nğŸš€ Executing research workflow (simulated)...")
            result = await self.system.conduct_research(question)
            
            # Display results
            print("\n" + "="*80)
            print("  Research Results (Simulated)")
            print("="*80)
            
            if result['success']:
                print(f"\nâœ… Workflow simulation completed!")
                print(f"ğŸ“ Project ID: {result['project_id']}")
                
                # Show project details
                if result.get('result'):
                    res = result['result']
                    if 'directory' in res:
                        print(f"ğŸ“‚ Directory: {res['directory']}")
                    if 'artifacts' in res:
                        print(f"ğŸ“„ Artifacts (simulated): {len(res['artifacts'])}")
                
                print(f"\nâš ï¸  Note: No actual files were created (simulation mode)")
                print(f"\nğŸ’¡ For real analysis, use:")
                print(f"   â€¢ analyze {question[:50]}...")
                print(f"   â€¢ chat (then discuss your research question)")
                
            else:
                print(f"\nâŒ Simulation failed")
                if result.get('error'):
                    print(f"Error: {result['error']}")
            
        except Exception as e:
            print(f"\nâŒ Error: {e}")
            import traceback
            traceback.print_exc()
    
    async def chat_mode(self):
        """Interactive chat mode"""
        print("\nğŸ’¬ Entering chat mode (type 'exit' to return)")
        print("="*80)
        
        if not self.llm_client:
            self.llm_client = create_llm_client()
        
        messages = []
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ You: ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'back']:
                    print("Exiting chat mode...\n")
                    break
                
                if not user_input:
                    continue
                
                messages.append({"role": "user", "content": user_input})
                
                print("ğŸ¤– QuARA: ", end="", flush=True)
                
                response = await self.llm_client.chat_completion(
                    messages=messages,
                    purpose=ModelPurpose.REASONING
                )
                
                print(response['content'])
                
                messages.append({"role": "assistant", "content": response['content']})
                
            except KeyboardInterrupt:
                print("\n\nExiting chat mode...\n")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}\n")
    
    async def ask_question(self, question: str):
        """Ask a single question"""
        if not self.llm_client:
            self.llm_client = create_llm_client()
        
        print(f"\nâ“ Question: {question}")
        print("\nğŸ¤– Answer: ", end="", flush=True)
        
        response = await self.llm_client.generate(question)
        print(response)
        print()
    
    async def generate_code(self, task: str):
        """Generate code for a task"""
        if not self.llm_client:
            self.llm_client = create_llm_client()
        
        print(f"\nğŸ’» Generating code for: {task}")
        print("="*80)
        
        code = await self.llm_client.generate_code(
            prompt=task,
            language="python"
        )
        
        print(f"\n{code}")
        print("\n" + "="*80)
    
    async def generate_writing(self, topic: str):
        """Generate academic writing"""
        if not self.llm_client:
            self.llm_client = create_llm_client()
        
        print(f"\nâœï¸  Generating content about: {topic}")
        print("="*80)
        
        content = await self.llm_client.write_content(
            prompt=topic,
            content_type="academic"
        )
        
        print(f"\n{content}")
        print("\n" + "="*80)
    
    async def analyze_topic(self, topic: str):
        """Perform comprehensive analysis of a topic"""
        if not self.llm_client:
            self.llm_client = create_llm_client()
        
        print(f"\nğŸ” Comprehensive Analysis: {topic}")
        print("="*80)
        
        # Multi-stage analysis
        stages = [
            ("ğŸ“š Background & Context", "Provide background and context for: "),
            ("ğŸ¯ Key Factors", "Identify and explain the key factors related to: "),
            ("ğŸ“Š Data & Evidence", "What data and evidence are relevant for analyzing: "),
            ("ğŸ’¡ Insights & Implications", "Provide insights and implications about: ")
        ]
        
        for stage_name, prompt_prefix in stages:
            print(f"\n{stage_name}")
            print("-" * 80)
            
            response = await self.llm_client.generate(
                f"{prompt_prefix}{topic}",
                temperature=0.7,
                purpose=ModelPurpose.REASONING
            )
            
            print(response)
        
        print("\n" + "="*80)
        print("âœ… Analysis complete!")
        print()
    
    async def automated_research(self, question: str):
        """Execute fully automated research with data download and code execution"""
        print(f"\nğŸš€ AUTOMATED RESEARCH: {question}")
        print("="*80)
        print("\nâœ¨ This will:")
        print("  1. Plan the research and identify data needs")
        print("  2. Download required financial data automatically")
        print("  3. Generate analysis code using LLM")
        print("  4. Execute the code and run statistical analysis")
        print("  5. Create visualizations (plots, charts)")
        print("  6. Generate a comprehensive research report")
        print("\nâ±ï¸  This may take 2-5 minutes...")
        print()
        
        try:
            # Initialize automated executor
            if not self.auto_executor:
                self.auto_executor = AutomatedResearchExecutor()
            
            # Execute automated research
            print("ğŸ”¬ Starting automated workflow...")
            results = await self.auto_executor.execute_research(question)
            
            # Display results
            print("\n" + "="*80)
            print("  AUTOMATED RESEARCH RESULTS")
            print("="*80)
            
            if results.get("success"):
                print(f"\nâœ… Research completed successfully!")
                print(f"\nğŸ“ Project: {results['project_id']}")
                print(f"ğŸ“‚ Directory: {results['directory']}")
                
                # Show what was created
                stages = results.get("stages", {})
                
                if "data_collection" in stages:
                    datasets = stages["data_collection"].get("datasets", {})
                    print(f"\nğŸ“Š Data Downloaded: {len(datasets)} datasets")
                    for ticker, info in datasets.items():
                        print(f"   â€¢ {ticker}: {info['rows']} rows ({info['date_range']})")
                
                if "analysis" in stages:
                    analysis = stages["analysis"]
                    if analysis["execution"]["success"]:
                        print(f"\nâœ… Analysis Code: Generated and executed successfully")
                        print(f"   Code: {analysis['code_file']}")
                    else:
                        print(f"\nâš ï¸  Analysis execution had issues")
                        if analysis["execution"]["stderr"]:
                            print(f"   Error: {analysis['execution']['stderr'][:200]}")
                
                if "visualizations" in stages:
                    viz = stages["visualizations"]
                    plots = viz.get("plots", [])
                    print(f"\nğŸ“ˆ Basic Visualizations: {len(plots)} plots created")
                    for plot in plots[:5]:  # Show first 5
                        print(f"   â€¢ {plot}")
                    if len(plots) > 5:
                        print(f"   ... and {len(plots) - 5} more")
                    
                    if not plots and viz.get("execution", {}).get("error"):
                        print(f"\nâš ï¸  Visualization generation had errors:")
                        print(f"   {viz['execution']['error'][:200]}")
                
                if "section_visualizations" in stages:
                    section_viz = stages["section_visualizations"]
                    section_plots = section_viz.get("plots", [])
                    print(f"\nğŸ¨ Section-Specific Visualizations: {len(section_plots)} plots created")
                    for plot in section_plots:
                        print(f"   â€¢ {plot}")
                    
                    if not section_plots and section_viz.get("execution", {}).get("error"):
                        print(f"\nâš ï¸  Section visualization generation had errors:")
                        print(f"   {section_viz['execution']['error'][:200]}")
                
                if "report" in stages:
                    report = stages["report"]
                    print(f"\nğŸ“ Report: {report['file']}")
                    print(f"\nPreview:")
                    print("-" * 80)
                    print(report.get("preview", ""))
                
                print(f"\nğŸ’¡ Next steps:")
                print(f"   â€¢ Open directory: cd {results['directory']}")
                print(f"   â€¢ View plots: open {results['directory']}/*.png")
                print(f"   â€¢ Read report: cat {results['directory']}/research_report.md")
                
            else:
                print(f"\nâŒ Research failed")
                if results.get("error"):
                    print(f"Error: {results['error']}")
            
        except Exception as e:
            print(f"\nâŒ Automated research failed: {e}")
            import traceback
            traceback.print_exc()
    
    async def list_projects(self):
        """List all research projects"""
        project_root = os.getenv('QUARA_PROJECT_ROOT', './research_projects')
        
        print(f"\nğŸ“ Research Projects in {project_root}:")
        print("="*80)
        
        if not os.path.exists(project_root):
            print("\nNo projects found. Start a new research project with 'research <question>'")
            return
        
        projects = [d for d in os.listdir(project_root) 
                   if os.path.isdir(os.path.join(project_root, d))]
        
        if not projects:
            print("\nNo projects found.")
        else:
            for i, project in enumerate(projects, 1):
                project_path = os.path.join(project_root, project)
                files = len([f for f in os.listdir(project_path) 
                           if os.path.isfile(os.path.join(project_path, f))])
                print(f"{i}. {project} ({files} files)")
        print()
    
    async def show_project_status(self, project_id: Optional[str] = None):
        """Show project status"""
        if not project_id:
            print("\nâŒ Please provide a project ID")
            print("Usage: status <project_id>")
            return
        
        if not self.system:
            self.system = QuARASystem()
            await self.system.start_system()
        
        print(f"\nğŸ“Š Project Status: {project_id}")
        print("="*80)
        
        try:
            status = await self.system.get_project_status(project_id)
            
            print(f"\n{'âœ…' if status['project_exists'] else 'âŒ'} Exists: {status['project_exists']}")
            print(f"ğŸ“‚ Directory: {status['project_directory']}")
            
            if status['artifacts']:
                print(f"\nğŸ“„ Artifacts ({len(status['artifacts'])}):")
                for artifact in status['artifacts']:
                    print(f"   - {artifact}")
            
            if status.get('memory_summary'):
                mem = status['memory_summary']
                print(f"\nğŸ§  Memory:")
                print(f"   Nodes: {mem.get('node_count', 0)}")
                print(f"   Types: {', '.join(mem.get('node_types', []))}")
            
        except Exception as e:
            print(f"\nâŒ Error: {e}")
        print()
    
    def show_models(self):
        """Show available models"""
        print("\nğŸ¤– Available Models")
        print("="*80)
        
        print("\nğŸ“‹ Configured Models:")
        print(f"   ğŸ§  Reasoning: deepseek-ai/DeepSeek-V3.2-Exp")
        print(f"      - Agent thinking, tool selection, decision making")
        print(f"      - Temperature: 0.7")
        
        print(f"\n   ğŸ’» Code: Qwen/Qwen2.5-Coder-32B-Instruct")
        print(f"      - Python/R/SQL generation, statistical analysis")
        print(f"      - Temperature: 0.3")
        
        print(f"\n   âœï¸  Writing: deepseek-ai/DeepSeek-V3.2-Exp")
        print(f"      - Academic writing, documentation")
        print(f"      - Temperature: 0.8")
        
        print("\nğŸ“š Other Available Models:")
        print("   - deepseek-ai/DeepSeek-V2.5")
        print("   - deepseek-ai/DeepSeek-Coder")
        print("   - Qwen/Qwen2.5-72B-Instruct")
        print()
    
    async def process_command(self, command: str):
        """Process a user command"""
        if not command.strip():
            return
        
        parts = command.strip().split(maxsplit=1)
        cmd = parts[0].lower()
        args = parts[1] if len(parts) > 1 else ""
        
        try:
            if cmd in ['exit', 'quit']:
                self.running = False
                print("\nğŸ‘‹ Thank you for using QuARA! Goodbye.\n")
                
            elif cmd == 'help':
                self.print_help()
                
            elif cmd == 'clear':
                os.system('clear' if os.name != 'nt' else 'cls')
                self.print_banner()
                
            elif cmd == 'config':
                self.print_config()
                
            elif cmd == 'models':
                self.show_models()
                
            elif cmd == 'test':
                await self.test_connection()
                
            elif cmd == 'research':
                if not args:
                    print("\nâŒ Please provide a research question")
                    print("Usage: research <your research question>")
                else:
                    await self.conduct_research(args)
            
            elif cmd == 'auto-research':
                if not args:
                    print("\nâŒ Please provide a research question")
                    print("Usage: auto-research <your research question>")
                else:
                    await self.automated_research(args)
                    
            elif cmd == 'validate':
                if not args:
                    print("\nâŒ Please provide a research question")
                else:
                    await self.validate_question(args)
                    
            elif cmd == 'estimate':
                if not args:
                    print("\nâŒ Please provide a research question")
                else:
                    time_est = estimate_research_time(args)
                    print(f"\nâ±ï¸  Estimated time: {time_est}")
                    
            elif cmd == 'chat':
                await self.chat_mode()
                
            elif cmd == 'ask':
                if not args:
                    print("\nâŒ Please provide a question")
                else:
                    await self.ask_question(args)
                    
            elif cmd == 'code':
                if not args:
                    print("\nâŒ Please describe what code to generate")
                else:
                    await self.generate_code(args)
                    
            elif cmd == 'write':
                if not args:
                    print("\nâŒ Please provide a topic to write about")
                else:
                    await self.generate_writing(args)
            
            elif cmd == 'analyze':
                if not args:
                    print("\nâŒ Please provide a topic to analyze")
                else:
                    await self.analyze_topic(args)
                    
            elif cmd == 'list':
                await self.list_projects()
                
            elif cmd == 'status':
                await self.show_project_status(args if args else None)
                
            else:
                print(f"\nâŒ Unknown command: {cmd}")
                print("Type 'help' for available commands")
                
        except Exception as e:
            print(f"\nâŒ Error executing command: {e}")
            import traceback
            traceback.print_exc()
    
    async def run_interactive(self):
        """Run interactive mode"""
        self.print_banner()
        
        # Check configuration
        if not os.getenv("SILICONFLOW_API_KEY"):
            print("âš ï¸  Warning: SILICONFLOW_API_KEY not set")
            print("Please configure your API key in .env file\n")
        
        while self.running:
            try:
                command = input("QuARA> ").strip()
                if command:
                    await self.process_command(command)
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye!\n")
                break
            except EOFError:
                print("\n\nğŸ‘‹ Goodbye!\n")
                break
        
        # Cleanup
        if self.system:
            await self.system.stop_system()
    
    async def run_command(self, args):
        """Run single command from CLI args"""
        if args.command == 'research':
            await self.conduct_research(args.question)
        elif args.command == 'chat':
            await self.chat_mode()
        elif args.command == 'config':
            self.print_config()
        elif args.command == 'test':
            await self.test_connection()
        elif args.command == 'models':
            self.show_models()
        
        # Cleanup
        if self.system:
            await self.system.stop_system()


async def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='QuARA - Quantitative Academic Research Agent CLI',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Add global --debug flag
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug logging for detailed execution traces'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Research command
    research_parser = subparsers.add_parser('research', help='Conduct research')
    research_parser.add_argument('question', help='Research question')
    
    # Chat command
    subparsers.add_parser('chat', help='Start chat session')
    
    # Config command
    subparsers.add_parser('config', help='Show configuration')
    
    # Test command
    subparsers.add_parser('test', help='Test LLM connection')
    
    # Models command
    subparsers.add_parser('models', help='Show available models')
    
    args = parser.parse_args()
    
    # Configure logging based on --debug flag
    import logging
    if args.debug:
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(levelname)s:%(name)s:%(message)s'
        )
        print("ğŸ› DEBUG MODE ENABLED - Detailed logging active\n")
    else:
        logging.basicConfig(
            level=logging.INFO,
            format='%(levelname)s:%(message)s'
        )
    
    cli = QuARACLI()
    
    if args.command:
        # Run specific command
        await cli.run_command(args)
    else:
        # Run interactive mode
        await cli.run_interactive()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Goodbye!\n")
        sys.exit(0)
